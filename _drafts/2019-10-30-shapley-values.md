---
title: "Diving Into Shapley Values and the SHAP library"
canonical_url: "https://edden-gerber.github.io/shapley-values/"
date: 2019-10-30
share: true
excerpt: "A deep empirical look into what we are asking when we ask for explanations using Shapley values"
header:
  image: "assets/images/shapley/header.png"
  teaser: "assets/images/shapley/header.png"
toc: true
toc_sticky: true
---
If you ever looked for a way to make your complex model's results more explainable, you probably encountered the idea of Shapley values. This is a popular and theoretically robust approach to quantifying how each feature contributes to a model's prediction for a particular sample. The main problem with Shapley values is the high cost of their computation (being exponential with the number of features). Fortunately, the powerful SHAP python library provides ways to sidestep this complexity and compute Shapley values efficiently.

My interest in this topic was sparked when I was using the SHAP library during a [recent hackathon](/datahack_2019/). I noticed that in the specific case we were working on, the SHAP code seemed to be quite inefficient - taking so long to run for the entire dataset, that I wondered whether in this case it would actually be faster to compute Shapley values using a direct, "brute force" approach. This led me to write a function that does exactly that, based on the original, inefficient formulation of Shapley values - and then test under which circumstances it can outperform the SHAP library. Along the way, I picked up some very interesting (in my opinion) insights about what exactly we are getting when we compute Shapley values using each approach.

## Outline and tl;dr
In this post I will try to do the following:
* **Explain what Shapley values are and how are they computed**
* **Show how the SHAP library and the "direct" Shapley computation provide two different approaches to feature importance, the former focusing on explaining individual samples given a model and the latter on explaining a model given a dataset.**
* **Show how under some (limited) circumstances, the direct Shapley computation can be more efficient than the SHAP library.**

(short summary of findings)

## What are Shapley Values
If you are familiar with the concept of Shapley values, you can skip this section. A quick Google search will also reveal numerous other sources online that explain this topic very well (I found that looking at the basic mathematical formulation on Wikipedia until I understood it gave me the best intuition), so my explanation will be brief.

Shapley values are a concept from game theory, describing how the contribution to a total payoff generated by a coalition of players is divided across the players. The relevance of this concept to machine learning is apparent if you translate "payoff" to "prediction" and "players" to "features".
The definition of the Shapley value (using ML terminology) is quite straightforward: the contribution of any feature is the expected value, across all possible permutations of feature combinations not containing this feature, of the prediction change caused by adding this feature. It looks like this:

{% include figure image_path="../assets/images/shapley/shapley-formula.png" alt="Shapley value formula" caption="_&phi;<sub>i</sub>_ is the Shapley value for feature _i_, _S_ is a coalition of features, _v(S)_ is the payoff for this coalition, and N is the total number of features. _N\\{i}_ is all the possible feature coalitions not containing _i_. The first term within the sum corresponds to the fraction of times _S_ appears within the possible feature permutations; intuitively, this gives the highest weight to the most informative contributions of a feature, i.e. when it is either isolated or when it is added to a full set of features. " %}

Let's look at an example: you have a model that predicts the probability of rain today, using three input Boolean features: A: is it winter?, B: is the sky cloudy?, and C: are some people carrying umbrellas? The model's output probability is a cumulative 20% for each individual positive feature, but if _A_ is true and any of the other features is true as well, the output probability is 90%. If today _A,B,_ and _C_ are all true - how much does each feature contribute to the final 90% output?

Let's compute this for feature _C_, using all possible feature combinations not containing it. For the sake of simplicity, let's say that each missing feature can be assumed to be _false_ rather than _unknown_.

|| S || v(S) || S &cup; {i} || v(S &cup; {i}) || Change in prediction after adding {i} ||
||---    ||: --- ||: --- ||: ---||: ---||
|| { }   || 0%      || {C}      || 20%   || **20%** ||
|| {A}   || 20%     || {A,C}    || 90%   || **70%** ||
|| {B}   || 20%     || {B,C}    || 40%   || **20%** ||
|| {A,B} || 90%     || {A,B,C}  || 90%   || **0%**  ||

Applying the formula (the first term of the sum in the Shapley formula is 1/3 for {} and {A,B} and 1/6 for {A} and {B}), we get a Shapley value of **21.66%** for feature _C_. Feature _B_ will naturally have the same value, while repeating this procedure for feature _A_ will give us **46.66%**. A vital characteristic of Shapley values is that the features' contributions always add up to the final score: **21.66% + 21.66% + 46.67% = 90%**.



##


 <font size="-1"> <b>If you wish to comment on this post you may do so on <a href="https://medium.com/@edden.gerber...">Medium</a>.</b> <font size="+1">
