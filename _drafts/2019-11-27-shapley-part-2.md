---
title: "A new perspective on Shapley values, part II: Radical Shapley values"
canonical_url: "https://edden-gerber.github.io/shapley-part-2/"
date: 2019-11-27
share: true
excerpt: "Exploring a conceptual alternative to SHAP"
header:
  image: "assets/images/shapley/header_2.png"
  teaser: "assets/images/shapley/header_2.png"
toc: true
toc_sticky: true
---

## Why should you read this post?
* **You want to better understand Shapley values and the SHAP tool**. Most other sources on these topics are explanations based on existing primary sources (e.g. academic papers and the SHAP documentation). This post is an attempt to gain some insights through an empirical approach.
* **You want to learn about an alternative approach to computing Shapley values**, that under some (limited) circumstances may be preferable to SHAP (or wait for the next post for a more broadly-applicable idea).

**If you are unfamiliar with Shaply values or SHAP**, or want a short recap of how the SHAP explainers work, [check out the previous post](/shapley-part-1/).

## A more wordy introduction
My interest in Shapley values was sparked when I was using the SHAP library during a [recent hackathon](https://edden-gerber.github.io/datahack-2019/) to explain the predictions of our Isolation Forest model. I noticed that for our model, the SHAP computation seemed to be quite inefficient, taking far too long to be run on the entire dataset. So long, in fact, that I wondered whether in this case the "brute force", exponentially-complex approach to Shapley values was actually a better option. This led me to write a function that computes Shapley values, using an approach that seemed intuitive to me - **instead of simulating missing features by integrating over their possible values, remove them altogether from the model training**. I decided to refer to this as **"radical" Shapley values**, in the sense that they are derived "from the root", that is, by re-training the model each time instead of relying on an existing model (if you can think of a better term I'd be happy to hear it!).

**The idea of this post is to compare the SHAP explainers to the radical Shapley approach** by seeing how they work in different scenarios (one thing I learned as a scientist: if you want to understand and trust your analysis tools, you need to subject them to the same type of rigorous empirical study as your actual object of research).

Code for the Shapley function and the examples used in this post is available [here](https://github.com/edden-gerber/radical-shapley-values).

## Outline (not quite a tl;dr)
In this post I will try to show the following:

* **Radical Shapley values can be computed for a low number of features** by retraining the model for each of 2<sup>M</sup> feature subsets.
* **The SHAP library explainers and the radical Shapley method provide two different interpretations to Shapley values**, the former best suited for explaining individual predictions for a given (trained) model, and the latter better suited for explaining global feature importance for a given dataset and model class.
* **Under some (limited) circumstances, the direct Shapley computation can be faster than the SHAP library explainers.** These circumstances are broadly when you **a.** have a low number of features (<~15), **b.** are using a model that is not supported by the efficient SHAP explainers and which has a relatively low training/prediction run time ratio (such as Isolation Forest), and **c.** need to compute Shapley values for a large number of samples (e.g., the entire dataset). In a future post I hope to eventually discuss a more practical polynomially-complex alternative of estimating radical Shapley values with sampling.

## So what are "radical" Shapley values?
A Shapley value reflects the expected value of the surplus payoff generated by adding a player to a coalition, across all possible coalitions that don't include the player (or, in the machine learning realm, the expected value of the difference in model output generated by adding a feature to the model). However, implementing the concept of Shapley values for explaining predictive models is matter of some interpretation. Specifically:
* **the SHAP explainers interpret "adding a feature" in terms of it having a specific value vs. being unknown, for a given sample, during the prediction phase**, while
* **the radical Shapley method is based on the alternative intuition of measuring a feature's impact in relation to it being absent from the model altogether during training**.

Both interpretations are consistent with the mathematical notion of Shapley values, but they measure slightly different things. The radical Shapley idea is not an entirely novel, of course, nor are these the only two possible interpretations of Shapley values for machine learning.

**The function for computing radical Shapley values (code [here](https://github.com/edden-gerber/radical-shapley-values)) takes a dataset and a payoff function, computes the payoff for each possible feature combination (or, "player coalition") and derives Shapley values** according to the formula:
{% include figure image_path="../assets/images/shapley/shapley-formula.png" alt="Shapley value formula" caption="_&phi;<sub>i</sub>_ is the Shapley value for feature _i_, _S_ is a coalition of features, _v(S)_ is the payoff for this coalition, and N is the total number of features. _N\\{i}_ is all the possible feature coalitions not containing _i_. The first term within the sum corresponds to the fraction of times _S_ appears within the possible feature permutations; intuitively, this gives the highest weight to the most informative contributions of a feature, i.e. when it is isolated or when it is added to a full set of features. " %}

**The payoff function can be any function that takes a dataset and returns a score** (for instance, the profit generated by a team of workers). It is thus a general function that can be used for any kind of Shapley computation, but for the purpose of generating radical Shapley values it will always be a function that trains a particular type of model on the dataset, and returns a prediction for each row.

**For example**: let's say we want to compute radical Shapley values for a model that predicts _y_ using _x<sub>1</sub>_, _x<sub>2</sub>_, and _x<sub>3</sub>_ with XGBoost. We will write a custom payoff function that initializes an XGB model, trains it on input arguments _X_ and _y_ and returns a prediction for each sample (perhaps splitting them non-randomly into training/validation and returning predictions for the validation only). The Shapley function will feed the payoff function each possible feature combination in _X_ - {_x<sub>1</sub>_}, {_x<sub>1</sub>_,_x<sub>2</sub>_}, etc. - and use the scores to compute a Shapley value for each feature and each sample. The output is then the same as that of the SHAP library explainers, and so all the SHAP plotting tools can be used to visualize it.

**The main disadvantage of this algorithm is its computational complexity** - it needs to run 2<sup>M</sup> times (where _M_ is the number of features), re-training the model each time. As a rule of thumb, if model training takes 1 second and you don't want more than about an hour of run time, you shouldn't use this method when you have more than 12 features. This complexity is of course the main reason the SHAP library was needed; on the other hand, under some limited circumstances this may be a faster option than using the SHAP Kernel explainer. **The issue of comparative run time is covered later in this post**.

## How is this different from SHAP, and why should we care?

I compared results from the radical Shapley method to both the SHAP KernelExplainer and TreeExplainer. I did not go into a comparison with the DeepExplainer, since neural network models rarely have the low number of input variables which would make the comparison relevant. As a quick summary, the radical Shapley method differs conceptually from all SHAP explainers by representing features' contribution to the model itself rather than to individual predictions; at the same time, it is generally slower (and impractical for more than a low number of features), although in some cases in may be more efficient than KernelExplainer.

### Radical Shapley vs. TreeExplainer

### Radical Shapley vs. KernelExplainer

## Why is KernelExplainer taking so long to run, and can radical Shapley be faster?
As we've established, the radical Shapely method needs to re-train the model and produce predictions 2<sup>num. features</sup> times. This makes it impractical whenever the number of features is not low (let's say, more than 15-20). But is it comparable to the SHAP explainers for a low number of features?

One thing to get out of the way is that **the optimized SHAP explainers will always be faster than the radical Shapley method, but the model-blind KernelExplainer can be very slow when explaining a large dataset**. To get a better idea, let's see what goes into the run time of the KernelExplainer vs the radical Shapley method:

|| In relation to... || The KernelExplainer run time is... || The radical Shapley run time is... ||
||---    ||: ---                 ||: ---            ||
|| Number of features   || fixed (aside from the model-dependent effect on prediction time) || **exponential** ||
|| Training time  || fixed || **linear** ||
|| Prediction time   || **linear (but computed ~200K times for each explained prediction)** || linear ||
|| Number of samples to explain || **linear** || fixed (aside from the model-dependent effect on train/predict time) ||

The bold entries emphasize the weaknesses of each method. The radical Shapley method is of course most vulnerable to increasing the number of features, and is also (linearly) slower with increased model training time, whereas KernelExplainer is not affected by these factors (although its prediction becomes more variable with increased number of features). The disadvantages of the KernelExplainer in terms of run time is that while it does not need to spend time re-training the model, it runs separately for each explained prediction (while radical Shapley runs for all predictions at once), each time having to produce predictions for ~200K samples (_nsamples_ X _num. background samples_, which by default are 2048+2M and 100, respectively).

**A good example for a model for which the radical Shapley method can perform faster than KernelExplainer, for a low number of features, is an _Isolation Forest_ model**, a popular tool for anomaly detection, as despite being a tree-based model it is not supported by TreeExplainer and its training time (compared to prediction) is relatively fast. To demonstrate this, I am using the [Credit Card Fraud Detection dataset from Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud), a ~285K sample, 30 features dataset used to predict anomalous credit card transaction. For our demonstration let's use 100K samples and  reduce the 30 features down to 15. On my old laptop I get the following approximate run times: <br>
_Train the model_:                              **8 sec** <br>
_Make predictions for all 100K samples_:        **8 sec** <br>
_Compute SHAP values for a single prediction_:  **18 sec** (which is about the time it takes to make predictions for ~200K bootstrapped samples...)<br>

Based on this we can make a rough estimate of how long it would take to compute Shapley values for the entire dataset. The KernelExplainer should simply take 100K x 18 seconds, or **about 500 hours**. The radical Shapley function will run for up to 2<sup>15</sup> x (15+13) seconds, or **about 150 hours** (actually, a better estimate may be about 50 hours since the dataset used for training in each iteration of the algorithm will have 1-14 features, or 7 on average). So both methods are slow although both could also be implemented with parallelization... Anyway, what's important here is not the specific example but understanding where the computation time comes from in each case. **If you need to explain only a small group of "important" predictions, KernelExplainer should be fast enough. If you need to explain a million predictions and you have less than 10-15 features, the radical Shapley method should be much faster.**

## A practical compromise? Estimating radical Shapley values with sampling
**Okay, but what if our model is not supported by TreeExplainer or DeepExplainer and we have too many features to compute radical Shapley values, but we really need Shapley values for our entire huge dataset?** I believe a relatively simple solution exists in this case, which is to estimate radical Shapley values using a sampling approach. Using random sampling to estimate Shapley values for a high number of players (as is done e.g. by the KernelExplainer) has been thoroughly discussed in the literature and improved methods are still being developed (see for example [Castro et al. 2009](https://www.sciencedirect.com/science/article/pii/S0305054808000804), [Castro et al. 2017](https://www.sciencedirect.com/science/article/pii/S030505481730028X) or [Benati et al. 2019](https://www.sciencedirect.com/science/article/abs/pii/S0377221719304448)). I would suggest that it can be beneficial to use sampling in combination with the radical Shapley approach - that is, to sample the space of feature combinations on which the model is trained (thus not needing to simulate missing features by averaging over bootstrapped samples).

The reason that this could be more efficient that using the KernelExplainer should be clear when looking at the table in the previous section: it eliminate the exponential computational complexity represented in the first row (by paying for it with increased estimation variance), leaving the dependence on training time the only computational disadvantage compared to the KernelExplainer, which in turn suffers from having to run the model a large number of times for every explained prediction. In the example given above, limiting the number of model re-training iterations to the same _nsamples_ parameters of the KernelExplainer would cut run time from 50-150 hours to about 10 hours (compared to KernelExplainer's ~500)- and crucially, **the sampling would assure that this would not increase significantly when increasing the number of features**

This is only a theoretical idea, and I would not make this post longer than it already is by developing it further. I would however be happy to get any comments you may about this (maybe you've already encountered this idea somewhere else?), and I hope to get a chance to complete this project and discuss it in a future post.
